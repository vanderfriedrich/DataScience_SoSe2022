{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\n# modules we'll use\nimport pandas as pd\nimport numpy as np\n\n# read in all our data\nnfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2016 (v3).csv\")\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\n\n# set seed for reproducibility\nnp.random.seed(0) \n\n\n\n# look at a few rows of the nfl_data file. I can see a handful of missing data already!\nnfl_data.sample(5)\n\n# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?\n\n# your code goes here :)\nsf_permits.sample(5)\n\n# get the number of missing data points per column\nmissing_values_count = nfl_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]\n\n# how many total missing values do we have?\ntotal_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n(total_missing/total_cells) * 100\n\n# your turn! Find out what percent of the sf_permits dataset is missing\nsf_missing_values_count = sf_permits.isnull().sum()\nsf_missing_values_count[0:10]\nsf_total_cells = np.product(sf_permits.shape)\nsf_total_missing = sf_missing_values_count.sum()\nprint('\\n Prozentzahl der fehlenden Daten ist: {}%'.format(sf_total_missing/sf_total_cells * 100))\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:10]\n\n#Your turn!¶\n#Look at the columns Street Number Suffix and Zipcode from the sf_permits datasets. Both of these contain missing values. \n#Which, if either, of these are missing because they don't exist? Which, if either, are missing because they weren't recorded?\nsf_missing_values_count[0:10]\n\n#Antwort: Die meisten Street Number Suffix sind nicht exisistend\n#Die PLZ sind nicht existend weil sie nicht aufgezeichnet wurden\n\n# remove all the rows that contain a missing value\nnfl_data.dropna()\n\n# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()\n\n# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])\n\n# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?\n\nsf_permits.dropna()\n\nsf_columns_with_na_dropped = sf_permits.dropna(axis=1)\nsf_columns_with_na_dropped.head()\n# Now try removing all the columns with empty values. Now how much of your data is left?\nprint(\"Reihen vor Löschung: %d \\n\" % sf_permits.shape[1])\nprint(\"Reihen mit na's dropped: %d\" % sf_columns_with_na_dropped.shape[1])\n\n# get a small subset of the NFL dataset\nsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data\n\n# replace all NA's with 0\nsubset_nfl_data.fillna(0)\n\n# replace all NA's the value that comes directly after it in the same column, \n# then replace all the reamining na's with 0\nsubset_nfl_data.fillna(method = 'bfill', axis=0).fillna(0)\n\n\n# Your turn! Try replacing all the NaN's in the sf_permits data with the one that\n# comes directly after it and then replacing any remaining NaN's with 0\nsf_permits.fillna(method='bfill',axis=0).fillna('0')\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-08T08:37:08.526662Z","iopub.execute_input":"2022-06-08T08:37:08.527041Z","iopub.status.idle":"2022-06-08T08:37:21.902414Z","shell.execute_reply.started":"2022-06-08T08:37:08.527012Z","shell.execute_reply":"2022-06-08T08:37:21.901449Z"},"trusted":true},"execution_count":12,"outputs":[]}]}